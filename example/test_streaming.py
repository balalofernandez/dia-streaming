import numpy as np
import soundfile as sf
import torch

from dia.model import Dia


# Initialize the Dia model
# Using float16 for faster inference on compatible hardware
model = Dia.from_pretrained("nari-labs/Dia-1.6B-0626", compute_dtype="float16")

# --- Test Case 1: Standard Streaming Generation (Not Compiled) ---
print("--- Running Test Case 1: Standard Streaming (Not Compiled) ---")

text = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."

full_audio = []
# Use the generate_streaming method to get a generator of audio chunks
for chunk in model.generate_streaming(
    text,
    use_torch_compile=False,  # Ensure we are testing the non-compiled version
    verbose=True,
    chunk_size=512,
    overlap=64,
):
    print(f"Received audio chunk of shape: {chunk.shape}")
    full_audio.append(chunk)

# Concatenate all the chunks to get the final audio
if full_audio:
    final_audio_np = np.concatenate(full_audio, axis=0)
    model.save_audio("test_streaming_standard.mp3", final_audio_np)
    print("\nStandard streaming test finished. Audio saved to test_streaming_standard.mp3\n")
else:
    print("\nStandard streaming test produced no audio.\n")


# # --- Test Case 2: Voice Cloning Streaming Generation ---
# print("--- Running Test Case 2: Voice Cloning Streaming ---")

# # The transcript for the audio prompt. This must accurately reflect the content of the audio.
# # We assume simple.mp3 has been generated by running simple.py
# clone_from_text = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."
# clone_from_audio = "simple.mp3"

# text_for_cloned_voice = "[S1] This audio should sound like the voice from the prompt. [S2] Let's see if the voice cloning worked correctly in streaming mode."

# full_cloned_audio = []
# # Call generate_streaming with the audio_prompt to enable voice cloning
# for chunk in model.generate_streaming(
#     clone_from_text + text_for_cloned_voice,
#     audio_prompt=clone_from_audio,
#     use_torch_compile=False,  # Can also be tested with True
#     verbose=True,
# ):
#     print(f"Received cloned audio chunk of shape: {chunk.shape}")
#     full_cloned_audio.append(chunk)

# if full_cloned_audio:
#     final_cloned_audio_np = np.concatenate(full_cloned_audio, axis=0)
#     model.save_audio("test_streaming_voice_clone.mp3", final_cloned_audio_np)
#     print("\nVoice cloning streaming test finished. Audio saved to test_streaming_voice_clone.mp3\n")
# else:
#     print("\nVoice cloning streaming test produced no audio.\n")


# # --- Test Case 3: Compiled Streaming Generation ---
# print("--- Running Test Case 3: Compiled Streaming ---")

# # Note: The first time you run with use_torch_compile=True, it will take a few minutes
# # to compile the model. Subsequent runs will be much faster.

# full_compiled_audio = []
# for chunk in model.generate_streaming(
#     text_to_generate,  # Using the same text as the first test case
#     use_torch_compile=True,  # Enable torch.compile
#     verbose=True,
# ):
#     print(f"Received compiled audio chunk of shape: {chunk.shape}")
#     full_compiled_audio.append(chunk)

# if full_compiled_audio:
#     final_compiled_audio_np = np.concatenate(full_compiled_audio, axis=0)
#     model.save_audio("test_streaming_compiled.mp3", final_compiled_audio_np)
#     print("\nCompiled streaming test finished. Audio saved to test_streaming_compiled.mp3\n")
# else:
#     print("\nCompiled streaming test produced no audio.\n")
